---
title: "STA610 Case Study 1"
author: "Emily Gentles, Weiyi Liu, Jack McCarthy, Qinzhe Wang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(janitor)
library(gridExtra)
library(cowplot)
library(knitr)
require(magrittr)
require(dplyr)
library(kableExtra)
library(readr)
library(tidyr)
library(broom)
library(lme4)
library(glmmTMB)
library(sjPlot)
library(brms)
library(coda)
library(rstan)
library(tidybayes)
library(naniar)
library(olsrr)
library(lmerTest)
library(rlist)
library(XML)
require(lattice)


# devtools::install_github("goodekat/redres")
library(redres)


```

```{r}
load('streetrx.RData')
```

Qinzhe - Coordinator & Checker: Double-checks the work for reproducibility and errors. Also responsible for submitting the report and presentation files.
         Coordinator: Keeps everyone on task and makes sure everyone is involved. Also responsible for coordinating team meetings and defining the objectives for each meeting.

Emily - Presenter: Primarily responsible for organizing and putting the team presentations together.

Jack - Programmer: Primarily responsible for all things coding. The programmer is responsible for putting everyone’s code together and making sure the final product is “readable”.

Weiyi - Writer: Primarily responsible for putting together the final report.

# Introduction


# EDA


### Missing Values

```{r, fig.width=6, fig.height=3.5}
na_check <- streetrx %>%
  filter(api_temp == 'morphine') %>%
  mutate_all( list( ~na_if(., '') ) ) %>%
  droplevels()
  
gg_miss_upset(na_check)
```

```{r}
# subset for group drug
morph_data <- streetrx %>%
  mutate_all( list( ~na_if(., '') ) ) %>%
  drop_na() %>%
  clean_names() %>%
  filter(api_temp == 'morphine') %>%
  mutate(
    quarter=substring(yq_pdate, 5, 5),
    year=substring(yq_pdate, 1, 4),
    state=recode_factor(droplevels(state), 'USA'='Unknown')
  )
```

### Response Distribution

First, a look at the distributions of the response variable "ppm". Observations 
with ppm between the 0.1 and 99.9 percentiles were considered so as to avoid 
the influence of extreme outliers on the analysis of the ppm distribution.

```{r, fig.width=10, fig.height=3.5}
# remove extreme outliers based on quantiles
morph_trunc <- morph_data %>%
  filter(between(ppm, quantile(ppm, 0.001), quantile(ppm, 0.999)))

# untransformed density
p1 <- morph_trunc %>%
  ggplot(aes(x=ppm)) +
    geom_histogram(
      aes(y=..density..), 
      color='black', 
      linetype='dashed',
      size=0.5,
      fill='lightblue', 
      alpha=0.5,
      bins=20
    ) +
    geom_density(size=0.75, bw=0.3) +
    labs(title='Distribution of morphine ppm') +
    theme_bw()

# log-transformed density
p2 <- morph_trunc %>%
  ggplot(aes(x=log(ppm))) +
    geom_histogram(
      aes(y=..density..), 
      color='black', 
      linetype='dashed',
      size=0.5,
      fill='lightcoral', 
      alpha=0.5,
      bins=20
    ) +
    geom_density(size=0.75, bw=0.3) +
    labs(title='Distribution of morphine log(ppm)') +
    xlim(-7, 3) +
    theme_bw()

grid.arrange(p1, p2, ncol=2)
```

The distribution of ppm is clearly right-skewed, and it is strictly nonnegative 
in value, so a log transformation may be appropriate. The distribution of 
log(ppm) is given above, and appears closer to the desired normal.

### state vs. log(ppm)

We see that there are 4 states that have a sample size of 1, North Dakota, Vermont, Washington DC, and Wyoming, as well as 1 state that has a sample size of 2, Alaska. Due to the extremely small sample sizes we decided to remove these states form our dataset to avoid computational instability.

```{r}
state_size <- morph_data %>%
  group_by(state) %>%
  summarise(n = n(), .groups = "drop")  %>%
  arrange(n) %>%
  pivot_wider(
    names_from=state,
    values_from=n
  )

state_size %>% 
  dplyr::select(1:7) %>%
  kable(
    caption = '7 States with Smallest Sample Size',
    align='c', 
    booktabs=TRUE) %>%
  kable_styling(latex_options = c('hold_position'))

state_size %>% 
  dplyr::select(44:50) %>%
  kable(
    caption='7 States with Largest Sample Size',
    align='c', 
    booktabs=TRUE
  ) %>%
  kable_styling(latex_options = c('hold_position'))
```

```{r}
# remove low sample size states
morph_data <- morph_data %>%
  mutate(state=as.character(state)) %>%
  filter(!state %in% c(
    'North Dakota', 'Washington, DC', 'Vermont', 'Wyoming', 'Alaska'
  ))
```


```{r, fig.width=10, fig.height=3}
morph_state <- morph_trunc %>%
  filter(state %in% state.name) %>%
  mutate(state_abv=state.abb[match(state,state.name)])

grand_mean <- mean(morph_state$ppm)

p3 <- morph_state %>%
  group_by(state_abv) %>%
  summarise(n = n(), mean = mean(ppm)) %>%
  ggplot(aes(x=n, y=mean)) +
    geom_hline(
      aes(yintercept=grand_mean),
      linetype='dashed',
      color='red',
      size=0.75
    ) +
    geom_point() +
    labs(x='sample size', y='mean log(ppm)') +
    theme_bw()

p4 <- morph_state %>%
  ggplot(aes(y=log(ppm), x=state_abv)) +
    geom_boxplot(
      fill=rainbow(49),
      alpha=0.5    
    ) +
    scale_x_discrete(guide=guide_axis(angle = 90)) +
    theme_bw() +
    labs(x='state')

cowplot::plot_grid(p3, p4, rel_widths = c(1, 2))
```

We observe that the within-state means for states with higher sample sizes in 
general adhere more closely to the grand mean. It is also evident that the log(ppm) 
distributions differ little as compared to the within-state variance. This is 
conducive to the borrowing of information between states.


### region vs. log(ppm)

We also have access to the broader region in which a purchase is made. This 
could be useful if we wanted to develop a simpler model that still captured 
variation by purchase location.

```{r, fig.height=3, fig.width=10}
t1 <- morph_state %>%
  group_by(usa_region) %>%
  summarise(n=n(), mean=round(mean(log(ppm)), 3)) %>%
  tableGrob()

p5 <- morph_state %>%
  ggplot(aes(y=log(ppm), x=usa_region)) +
    geom_boxplot() +
    stat_summary(
      fun.y=mean, 
      geom='point',
      color='red',
      size=3
    ) +
    theme_bw()

grid.arrange(t1, p5, ncol=2, widths=c(2, 2))
```


### source vs. log(ppm)

```{r, fig.width=6, fig.height=3.5}
# unique(morph_data$source)

# combine internet levels into single level
morph_data <- morph_data %>%
  mutate(source=replace(
    source, !source %in% c(
      'Personal',
      'Heard it', 
      'Internet', 
      'Internet Pharmacy', 
      'Drug forum', NA
    ), 'Internet'
  )) %>%
  droplevels()

morph_data %>%
  ggplot(aes(x = source,y = log(ppm))) +
  geom_boxplot() +
  stat_summary(
    fun.y=mean, 
    geom='point',
    color='red',
    size=3
  ) +
  theme_bw()
```


### date

record `price_date` as a continuous variable counting days from some start date.

```{r}
#min(as.Date(morph_data$price_date, "%m/%d/%y")) #2013-01-01

morph_data <- morph_data %>% 
  mutate(date_diff = as.numeric(
    as.Date(morph_data$price_date, "%m/%d/%y") - as.Date("2013-01-01")
  ))
```

```{r}
morph_data %>%
  ggplot(aes(x=date_diff)) +
    geom_histogram(
      aes(y=..density..), 
      color='black', 
      linetype='dashed',
      size=0.5,
      fill='lightblue', 
      alpha=0.5,
      bins=30
    ) +
    geom_density(size=0.75, bw=100) +
    labs(title='Date Distribution') +
    theme_bw()
```



### year & quarter vs.log(ppm)

```{r,fig.width=10, fig.height=3.5}
# t_year <- morph_data %>% 
#   group_by(year) %>% 
#   summarize(n = n()) %>% 
#   tableGrob()

yearplot <- morph_data %>%
  ggplot(aes(x = year,y = log(ppm))) +
  geom_boxplot() +
  labs(x='Year') +
  stat_summary(
    fun.y=mean, 
    geom='point',
    color='red',
    size=3
  ) +
  theme_bw()

# t_quarter <- morph_data %>% 
#   group_by(quarter) %>% 
#   summarize(n = n()) %>% 
#   tableGrob()

quarterplot <- morph_data %>%
  ggplot(aes(x = quarter,y = log(ppm))) +
  geom_boxplot() +
  labs(
    x='Quarter',
    y=''
  )  +
  stat_summary(
    fun.y=mean, 
    geom='point',
    color='red',
    size=3
  ) +
  theme_bw()

# grid.arrange(t_year, yearplot, t_quarter, quarterplot, ncol=2, nrow = 2)
grid.arrange(yearplot, quarterplot, ncol=2)

# jpeg("EDAplotQuarter.jpg", width = 500)
# dev.off()
```

### bulk_purchase vs.log(ppm)

```{r, fig.width=6, fig.height=3.5}
morph_data %>%
  ggplot(aes(x = bulk_purchase,y = log(ppm))) +
  geom_boxplot() +
  labs(x='Bulk Purchase') +
  stat_summary(
    fun.y=mean, 
    geom='point',
    color='red',
    size=3
  ) +
  theme_bw()

# jpeg("EDAplotBulk.jpg", width = 500)
# dev.off()
```


### Primary_Reason vs.log(ppm)


```{r}
morph_data %>%
  ggplot(aes(x = primary_reason,y =log(ppm))) +
  geom_boxplot() +
  coord_flip() +
  labs(x = "log(ppm)", y = "Reason")  +
    stat_summary(
      fun.y=mean, 
      geom='point',
      color='red',
      size=3
    )

# jpeg("EDAplotReason.jpg", width = 500)
# dev.off()
```


### mgstr vs. log(ppm)

```{r}
morph_data %>%
  ggplot(aes(x=mgstr, y=log(ppm))) +
    geom_point() +
    geom_smooth() +
    theme_bw()

morph_data %>%
  ggplot(aes(x=log(mgstr), y=log(ppm))) +
    geom_point() +
    geom_smooth() +
    theme_bw()

morph_data %>%
  ggplot(aes(x=mgstr)) +
    geom_histogram(
      aes(y=..density..), 
      color='black', 
      linetype='dashed',
      size=0.5,
      fill='lightblue', 
      alpha=0.5,
      bins=10
    ) +
    geom_density(size=0.75, bw=7.5) +
    labs(title='Date Distribution') +
    theme_bw()
```

```{r}
morph_data %>% 
  group_by(mgstr) %>% 
  summarize(n = n()) %>%
  pivot_wider(
    names_from=mgstr,
    values_from=n
  ) %>%
  kable(
    caption='Sample Size for mgstr Levels',
    align='c', 
    booktabs=TRUE
  ) %>%
  kable_styling(latex_options = c('hold_position'))

# inspect mgstr value quantiles
quantile(morph_data$mgstr, c(0.25, 0.5, 0.75)) %>%
  data.frame() %>%
  rename('mgstr'='.') %>%
  kable()

## here we decide to re-code mgstr by quantile
morph_data <- morph_data %>%
  mutate(mgstr2 = case_when(
    mgstr <= 15              ~ "low",
    mgstr >15 & mgstr <= 30  ~ "medium",
    mgstr >30 & mgstr <= 50  ~ "medium high",
    mgstr >=50               ~ "high")
  )

morph_data %>%
  ggplot(aes(x=mgstr2 ,y=log(ppm))) +
  geom_boxplot() +
  labs(y="log(ppm)", x="Strength")  +
  coord_flip() +
  stat_summary(
    fun.y=mean, 
    geom='point',
    color='red',
    size=3
  ) +
  theme_bw()

# jpeg("EDAplotStrength.jpg", width = 500, height = 300)
# dev.off()
```





# Model

```{r}
modela <- lmer(log(ppm) ~ date_diff + (1|state), data = morph_data)
modelb <- lmer(log(ppm) ~ date_diff + quarter + (1|state), data = morph_data)
modelc <- lmer(log(ppm) ~ date_diff + quarter + mgstr + (1|state), data = morph_data)
modeld <- lmer(log(ppm) ~ date_diff + quarter + mgstr + bulk_purchase + (1|state), data = morph_data)
modele <- lmer(log(ppm) ~ date_diff + quarter + mgstr + bulk_purchase + primary_reason + (1|state), data = morph_data)
modelf <- lmer(log(ppm) ~ date_diff + quarter + mgstr + bulk_purchase + primary_reason + source + (1|state), data = morph_data)

#m <- ols_step_best_subset(model)
#plot(m)
#selectedMod <- step(model)
#summary(selectedMod)

bic_score <- sapply(c(modela, modelb, modelc, modeld, modele, modelf), BIC)
re_results <- data.frame('Grouping' = c(
    'Basic', '\\+ Source', '\\+ Reason', '\\+ Bulk', '\\+ mgstr', '\\+ quarter'
  ), 'BIC' = bic_score)
kable(re_results)
```
From this it looks like the best model includes date_diff, quarter, and mgstr 



### choose grouping variable


```{r}

# group by state
mod_1 <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr +
                bulk_purchase + primary_reason + source, REML=F)

# group by city
mod_2 <- lmer(data=morph_data, log(ppm) ~ (1 |city) + date_diff + quarter + mgstr +
                bulk_purchase + primary_reason + source, REML=F)

# group by region
mod_3 <- lmer(data=morph_data, log(ppm) ~ (1 |usa_region) + date_diff + quarter + mgstr +
                bulk_purchase + primary_reason + source, REML=F)

bic_score <- sapply(c(mod_1, mod_2, mod_3), BIC)
data.frame('Grouping' = c('State', 'City', 'Region'), 'BIC' = bic_score) %>%
  kable()
 
# jpeg("BICgroup.jpg", width = 350)
# dev.off()
```

Choose `State` as our grouping variable


```{r}
# raw mgstr values
mod_4 <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr +
                bulk_purchase + primary_reason + source, REML=F)

# new mgstr levels
mod_5 <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr2 +
                bulk_purchase + primary_reason + source, REML=F)

# basic model with raw mgstr
mod_6 <- lmer(data=morph_data, log(ppm) ~ (1 |state) +  mgstr, REML=F)

# basic model with new mgstr levels
mod_7 <- lmer(data=morph_data, log(ppm) ~ (1 |state) +  mgstr2, REML=F)

sapply(c(mod_4, mod_5, mod_6, mod_7), BIC)

step(mod_5)
```



```{r}
# Test date_num
mod_drop_date <- lmer(data=morph_data, log(ppm) ~ (1 |state)  + quarter + mgstr2
                      + bulk_purchase + primary_reason + source, REML=F)


mod_drop_quater <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff  + mgstr2
                        + bulk_purchase + primary_reason + source, REML=F)

mod_drop_bulk_purchase <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr2 
                               + primary_reason + source, REML=F)

mod_drop_primary_reason <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr2
                                + bulk_purchase + source, REML=F)

mod_drop_mgstr <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter
                       + bulk_purchase + primary_reason + source, REML=F)


mod_drop_source <- lmer(data=morph_data, log(ppm) ~ (1 |state) + date_diff + quarter + mgstr2
                        + bulk_purchase + primary_reason, REML=F)


anova(mod_4,mod_drop_date)$`Pr(>Chisq)`[2] 
anova(mod_4,mod_drop_quater)$`Pr(>Chisq)`[2] # add
anova(mod_4,mod_drop_bulk_purchase)$`Pr(>Chisq)`[2] # add
anova(mod_4,mod_drop_primary_reason)$`Pr(>Chisq)`[2] # add
anova(mod_4,mod_drop_mgstr)$`Pr(>Chisq)`[2] # add
anova(mod_4,mod_drop_source)$`Pr(>Chisq)`[2] 
```


We now have `quarter`, `bulk_purchase`, `primary_reason` and `mgstr2` in our model, regarding `state` as the grouping variable.

Why/how did we choose the variables to put into the model? too many predictors?

Unique values: 
state = 45
quarter = 4
bulk purchase = 2
primary reason = 10
mgstr = 14

only have 1831 observations

### final model


```{r, eval =T}
mod_final1 <- lmer(data=morph_data, log(ppm) ~ (1 |state)  + mgstr + quarter
                   + bulk_purchase + primary_reason, REML=F)

mod_final2<- lmer(data=morph_data, log(ppm) ~ (1 |state)  + mgstr2 + quarter
                  + bulk_purchase + primary_reason, REML=F)

BIC(mod_final1)
BIC(mod_final2)
```

```{r}
# view coefficient estimates
view_coef <- function(model) {
  summary(model)$coefficients %>%
    as.data.frame() %>%
    mutate(`exp(Estimate)`=exp(Estimate)) %>%
    relocate(`exp(Estimate)`, .after=Estimate) %>%
    kable() %>%
    kable_classic(full_width=FALSE)
}

# view parameter estimates
view_params <- function(model) {
  params <- summary(model)$varcor %>%
    as.data.frame() %>%
    select(vcov)
  rownames(params) <- c('$\\tau^2$', '$\\sigma^2$')
  colnames(params) <- c('Estimate')
  kable(params) %>% 
    kable_classic(full_width=FALSE)
}
```

```{r, fig.width=10, fig.height=10}
plot_qq <- function(model) {
  df <- data.frame(
    res=residuals(model, scaled=TRUE)
  )
  
  p <- ggplot(df, aes(sample=res)) + 
    stat_qq() + 
    stat_qq_line(
      linetype='dashed',
      color='red',
      size=1
    ) +
    labs(
      title='Normal QQ for Residuals',
      x='Theoretical Quantiles',
      y='Standardized Residuals'
    ) + theme_bw()
  
  return(p)
}
plot_ranef_qq <- function(model) {
  df <- data.frame(
    res=ranef(model)[[1]][[1]]
  )
  
  p <- ggplot(df, aes(sample=res)) + 
    stat_qq() + 
    stat_qq_line(
      linetype='dashed',
      color='red',
      size=1
    ) +
    labs(
      title='Normal QQ for Random Effects',
      x='Theoretical Quantiles',
      y='State Intercept'
    ) + theme_bw()
  
  return(p)
}
plot_res_fit <- function(model) {
  df <- data.frame(
    res=residuals(model),
    fit=fitted(model)
  )
  
  p <- ggplot(df, aes(x=fit, y=res)) +
    geom_point() +
    geom_hline(
      yintercept=0,
      linetype="dashed"
    ) +
    geom_smooth() +
    labs(
      title='Residuals vs. Fitted',
      x='Fitted',
      y='Residuals'
    ) + theme_bw()
  
  return(p)
}
plot_scale_loc <- function(model) {
  df <- data.frame(
    res=sqrt(residuals(model, scaled=TRUE)),
    fit=fitted(model)
  )
  
  p <- ggplot(df, aes(x=fit, y=res)) +
    geom_point() +
    geom_smooth() +
    labs(
      title='Scale-Location',
      x='Fitted',
      y=expression(sqrt('Standardized Residuals'))
    ) + theme_bw()
  
  return(p)
}
plot_res_dens <- function(model) {
  df <- data.frame(
    res=residuals(model)
  )
  
  p <- ggplot(df, aes(x=res)) +
    geom_density() +
    labs(
      title='Residuals Density',
      x='Residuals',
      y='Density'
    ) + theme_bw()
  
  return(p)
}
  
model_diag <- function(model) {
  p1 <- plot_res_fit(model)
  p2 <- plot_qq(model)
  p3 <- plot_ranef_qq(model)
  p4 <- plot_res_dens(model)
  
  cowplot::plot_grid(p1, p2, p3, p4, nrow=2)
}
```

```{r, fig.heigh=10, fig.width=10}
view_coef(mod_final2)
view_params(mod_final2)
model_diag(mod_final2)
```



Remove the data point with the lowest residual.

```{r}
# remove lowest residual data point
morph_data2 <- morph_data[-which.min(resid(mod_final2)),]

# re-fit final model
mod_final3 <- lmer(data=morph_data2, log(ppm) ~ (1 |state)  + mgstr2
                   + bulk_purchase + quarter + primary_reason , REML=F)

view_coef(mod_final3)
view_params(mod_final3)
model_diag(mod_final3)
```


# Influence

```{r}
mod_final3_inf<- influence(mod_final3, group = "state")
```

```{r}
cooks_distance <- cooks.distance(mod_final3_inf)
cutline <- 4 / length(unique(morph_data2$state))
infindiv <- cooks_distance > cutline

ggplot(data=NULL, aes(x=1:length(unique(morph_data2$state)), y=cooks_distance)) +
  geom_point() +
  geom_hline(
    yintercept=cutline,
    linetype='dashed',
    color='red',
    size=0.75
  ) +
  labs(
    x='Index',
    y='Cooks Distance'
  ) +
  theme_bw()

# jpeg("CooksDistance.jpg", width = 600)
# dev.off()

data.frame(
  rownames(mod_final3_inf$`fixed.effects[-state]`),
  cooks_distance,
  infindiv
) %>% 
  filter(infindiv == TRUE) %>%
  select(1:2) %>%
  rename(`Influential State`=1) %>%
  kable() %>%
  kable_classic(full_width=FALSE)
```


```{r}
# remove two most influential states
morph_data3 <- morph_data2 %>%
  filter(!state %in% c('Arizona', 'California'))

mod_final4 <- lmer(data=morph_data3, log(ppm) ~ (1 |state)  + mgstr2
                   + bulk_purchase + quarter + primary_reason, REML=F)

view_coef(mod_final4)
view_params(mod_final4)
model_diag(mod_final4)
```

```{r}
# view intercept estimates and intervals
dotplot(ranef(mod_final2, condVar = TRUE))

# jpeg("randomef.png", width = 350)
# dev.off()
```


Plots are not good -> not remove those two states?





Interclass correlation is 0.0159, very small so very little correlation across states.
Including bulk purchases, the interclass correlation is 0.016, so bulk purchase actually increases the heterogeneity across states by a very small amount.

Make table with results for all models tested in ANOVA




